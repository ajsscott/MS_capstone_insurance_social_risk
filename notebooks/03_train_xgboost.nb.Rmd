---
title: "XGBoost Modeling R Notebook"
author: "AJ Strauman-Scott"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(caret)
library(xgboost)
library(SHAPforxgboost)
library(Matrix)
library(reticulate)
library(doParallel)
```

## LOAD DATA

```{r load-data}
df <- readRDS("../data/models/social-risk-crash-rate-data.rds")
```

## HOT-ENCODE `YEAR`

I will treat `year` as a factor and one-hot encode it.

```{r hot-encode-year}
df$year <- as.factor(df$year)
year_dummies <- model.matrix(~ year - 1, data = df)

df <- cbind(df[ , !(names(df) %in% c("year"))], year_dummies)
```

## PREPARE TARGET

We will remove all possible target variables and keep only one per model training.

```{r single-target-df}
# Choose your target variable (e.g., crash rate per 1,000 residents)
target_var <- "crash_rate_per_1000"

# Remove all target variables except selected
cols_to_remove <- grep("per_1000", 
                       names(df), 
                       value = TRUE)
cols_to_remove <- setdiff(cols_to_remove, target_var) # keep this column

df <- df %>% select(-all_of(cols_to_remove),)

# Create feature matrix and target vector
X <- df %>% select(-target_var, -borough, -total_population, -geoid)
y <- df[[target_var]]

glimpse(X)
```


## HYPERPARAMETER TUNING

**What This Does**
- Optuna (Python) handles the search space and Bayesian optimization.
- The final best parameters are applied to fit the `final_model`.
- **Search space:** Instead of predefined grids, `trial$suggest_float()` and `trial$suggest_int()` explore a range of values.
- **Best parameters:** `study$best_params` holds the optimal hyperparameters.

```{r tune-params, eval=FALSE}
## CONVERT TO DMATRIX
dtrain_all <- xgb.DMatrix(data = as.matrix(X), label = y)

## Start Python venv
reticulate::use_virtualenv("r-reticulate", required = TRUE)

## OPTUNA-BASED SPATIAL CV
optuna <- import("optuna")

boroughs <- unique(df$borough)
folds <- lapply(boroughs, function(b) which(df$borough != b))

# Optuna objective
objective <- function(trial) {
  params <- list(
    booster = "gbtree",
    eta = trial$suggest_float("eta", 0.01, 0.3, log = TRUE),
    max_depth = trial$suggest_int("max_depth", 3, 12),
    min_child_weight = trial$suggest_int("min_child_weight", 1, 10),
    subsample = trial$suggest_float("subsample", 0.5, 1.0),
    colsample_bytree = trial$suggest_float("colsample_bytree", 0.5, 1.0),
    gamma = trial$suggest_float("gamma", 0, 10),
    lambda = trial$suggest_float("lambda", 0, 10),
    alpha = trial$suggest_float("alpha", 0, 10)
  )
  
  rmse_scores <- numeric(length(folds))
  for (i in seq_along(folds)) {
    train_idx <- folds[[i]]
    valid_idx <- setdiff(seq_len(nrow(dtrain_all)), train_idx)

    dtrain <- xgb.DMatrix(data = as.matrix(X[train_idx, ]), label = y[train_idx])
    dvalid <- xgb.DMatrix(data = as.matrix(X[valid_idx, ]), label = y[valid_idx])

    model <- xgb.train(
      params = params,
      data = dtrain,
      nrounds = 500,
      watchlist = list(val = dvalid),
      early_stopping_rounds = 20,
      verbose = 0
    )

    rmse_scores[i] <- min(model$evaluation_log$val_rmse)
  }
  
  preds <- predict(model, as.matrix(X[valid_idx, ]))
  return(Metrics::rmse(y[valid_idx], preds))
}

# Run Optuna study
set.seed(2025)
study <- optuna$create_study(direction = "minimize")
study$optimize(objective, n_trials = 50)

best_params <- study$best_params
print(best_params)
```
$eta
[1] 0.2309768

$max_depth
[1] 4

$min_child_weight
[1] 3

$subsample
[1] 0.5583975

$colsample_bytree
[1] 0.8797697

$gamma
[1] 3.35839

$lambda
[1] 6.131609

$alpha
[1] 2.805485

## TRAIN/TEST SPLIT

```{r train-test-split}
# Set seed
set.seed(2025)

# Split by index
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_index, ]
y_train <- y[train_index]
X_test <- X[-train_index, ]
y_test <- y[-train_index]

# Convert to xgb.DMatrix
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)
```

## TRAIN MODEL

```{r train-model, eval=FALSE}
# Set seed
set.seed(2025)

# Training with parallel processing
final_model <- xgb.train(
  params = list(
    eta = best_params$eta,
    max_depth = best_params$max_depth,
    gamma = best_params$gamma,
    colsample_bytree = best_params$colsample_bytree,
    min_child_weight = best_params$min_child_weight,
    subsample = best_params$subsample,
    objective = "reg:squarederror",
    eval_metric = "rmse"
  ),
  data = dtrain,
  nrounds = 1000,
  watchlist = list(train = dtrain, test = dtest),
  early_stopping_rounds = 20,
  verbose = 1,
  nthread = detectCores() - 1
)
```
[78]	train-rmse:1.220267	test-rmse:1.555561 
[79]	train-rmse:1.211549	test-rmse:1.556347 
[80]	train-rmse:1.207152	test-rmse:1.553078 
[81]	train-rmse:1.205943	test-rmse:1.553145 
[82]	train-rmse:1.205311	test-rmse:1.552420 
[83]	train-rmse:1.201741	test-rmse:1.555835 
[84]	train-rmse:1.197931	test-rmse:1.553410 
[85]	train-rmse:1.191889	test-rmse:1.551760 
[86]	train-rmse:1.189975	test-rmse:1.551783 
[87]	train-rmse:1.188499	test-rmse:1.550743 
Stopping. Best iteration:
[67]	train-rmse:1.258358	test-rmse:1.549545
## SAVE MODEL

```{r save-model, eval=FALSE}
# Create directory if it doesn't exist
if (!dir.exists("../data/models")) {
  dir.create("../data/models", recursive = TRUE)
}

# Save the final XGBoost model
saveRDS(final_model, file = "../data/models/xgb_model.rds")

# Save the best parameters
saveRDS(best_params, file = "../data/models/xgb_best_params.rds")

cat("Model and parameters saved to ../data/models/")
```

```{r load-model}
# Load the final XGBoost model
final_model <- readRDS("../data/models/xgb_model.rds")

# Load the best parameters
best_params <- readRDS("../data/models/xgb_best_params.rds")

```

## MODEL EVALUATION

```{r eval-model, message=FALSE, warning=FALSE}
library(Metrics)
library(ggplot2)
library(dplyr)

set.seed(2025)

# Predict on test set
preds <- predict(final_model, as.matrix(X_test))

# --- Metrics ---
rmse <- sqrt(mean((y_test - preds)^2))
mae <- mean(abs(y_test - preds))
mape <- mean(abs((y_test - preds) / y_test)) * 100
r2 <- 1 - (sum((y_test - preds)^2) / sum((y_test - mean(y_test))^2))

cat("Model Evaluation Metrics:\n")
cat("  RMSE:", rmse, "\n")
cat("  MAE :", mae, "\n")
cat("  MAPE:", mape, "%\n")
cat("  R²  :", r2, "\n\n")

# --- Residuals ---
residuals <- y_test - preds
residual_df <- data.frame(
  actual = y_test,
  predicted = preds,
  residuals = residuals
)

# --- Plot: Predicted vs Actual ---
p1 <- residual_df %>%
  ggplot(aes(x = actual, y = predicted)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  theme_minimal() +
  labs(title = "Predicted vs Actual Crash Rates",
       x = "Actual",
       y = "Predicted")

# --- Plot: Residuals vs Predicted ---
p2 <- residual_df %>%
  ggplot(aes(x = predicted, y = residuals)) +
  geom_point(alpha = 0.5, color = "blue") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  theme_minimal() +
  labs(title = "Residuals vs Predicted",
       x = "Predicted",
       y = "Residuals")

# --- Plot: Residual Density ---
# Residual Histogram
p3 <- ggplot(residual_df, aes(x = residuals)) +
    geom_histogram(binwidth = 0.2, fill = "steelblue", color = "white") +
    theme_minimal() +
    labs(title = "Residual Distribution", x = "Residuals", y = "Count")

# Print plots
print(p1)
print(p2)
print(p3)

ggsave("../report/plots/predicted_vs_actual_values_plot.png", p1, width = 10, height = 6, dpi = 300)
ggsave("../report/plots/resisuals_vs_predicted_values_plot.png", p2, width = 10, height = 6, dpi = 300)
ggsave("../report/plots/residual_density_plot.png", p3, width = 10, height = 6, dpi = 300)
```
Model Evaluation Metrics:
  RMSE: 1.549545 
  MAE : 0.8372956 
  MAPE: Inf %
  R²  : 0.2595503 
## SHAP EXPLAINABILITY

```{r shap}
# Compute SHAP values
shap_values <- shap.values(xgb_model = final_model, X_train = as.matrix(X_train))
shap_long <- shap.prep(shap_contrib = shap_values$shap_score, X_train = as.matrix(X_train))

# SHAP summary plot
print(shap.plot.summary(shap_long))

if (!dir.exists("../report/plots")) {
  dir.create("../report/plots")
}

shap <- shap.plot.summary(shap_long)
ggsave("../report/plots/shap_summary_plot.png", shap, width = 10, height = 6, dpi = 300)
```

## INSPECT TREE ENSEMBLE

```{r inspect-tree}
xgb.plot.tree(model = final_model, trees = 0)
```
```{r}
xgb.plot.tree(model = final_model, trees = 1)
```
```{r}
xgb.plot.tree(model = final_model, trees = 2)
```
```{r}
xgb.plot.multi.trees(model = final_model)
```

```{r}
# ============================================================
# Additional Model Diagnostics and Deeper Analysis
# ============================================================

library(ggplot2)
library(dplyr)
library(pdp)        # For Partial Dependence Plots
library(DALEX)      # For model explainability
library(ggthemes)
library(sf)

# ---------------------------
# 1. SHAP Dependence and Interaction Plots
# ---------------------------
message("\nGenerating SHAP dependence and interaction plots...")

# Assuming shap_values and shap_long are already computed
# (if not, recompute them using iml or SHAPforxgboost packages)

# Top feature by SHAP importance
top_feature <- shap_long %>%
  as_tibble() %>%
  count(variable, wt = abs(value), sort = TRUE) %>%
  dplyr::slice(1) %>%
  pull(variable)

# Dependence plot for top feature
shap.plot.dependence(data_long = shap_long, x = top_feature, color_feature = top_feature)

# Interaction values
shap_interaction_values <- predict(
  final_model,
  as.matrix(X_train),
  predinteraction = TRUE
)

# shap_interaction_values will be a 3D array: [n_samples, n_features, n_features]
interactions <- dim(shap_interaction_values)
saveRDS(interactions, "../data/models/shap_interactions.rds")
```

```{r}
# ---------------------------
# 3. Partial Dependence Plots (PDP)
# ---------------------------
message("\nGenerating Partial Dependence Plots...")

# Top variables (manually set order as per user request)
top_features <- c(
  "post_pandemic",
  "pct_age_65_plus",
  "pct_hs_diploma",
  "median_gross_rent",
  "pct_in_labor_force",
  "poverty_vehicle_interaction"
)

# Custom display names
feature_names <- c(
  "Pre vs Post Pandemic",
  "% over 65 years old",
  "% with a High School Diploma Only",
  "Median Gross Rent",
  "% in the Labor Force",
  "Poverty x Vehicle Ownership"
)

# Ensure the output directory exists
dir.create("../report/plots/pdp", recursive = TRUE, showWarnings = FALSE)

# Generate individual PDP plots
library(pdp)
library(ggplot2)
library(gridExtra)

pdp_plots <- list()
for (i in seq_along(top_features)) {
  f <- top_features[i]
  pd <- partial(final_model, pred.var = f, train = as.matrix(X_train), grid.resolution = 30)
  
  p <- autoplot(pd) +
    ggtitle(feature_names[i]) +
    theme_minimal() +
    theme(
      axis.title = element_blank(),
      axis.text = element_blank(),
      axis.ticks = element_blank(),
      plot.title = element_text(size = 12, hjust = 0.5)
    )
  
  pdp_plots[[i]] <- p
  
  # Save individual plot
  ggsave(
    filename = paste0("../report/plots/pdp/pdp_", f, ".png"),
    plot = p,
    width = 5,
    height = 4,
    dpi = 300
  )
}

# Arrange all PDP plots into a grid and save
grid_plot <- gridExtra::grid.arrange(grobs = pdp_plots, ncol = 2)

ggsave(
  filename = "../report/plots/pdp/pdp_grid.png",
  plot = grid_plot,
  width = 10,
  height = 12,
  dpi = 300
)

message("PDP grid saved to ../report/plots/pdp/pdp_grid.png")

```
