---
title: "XGBoost Modeling R Notebook"
author: "AJ Strauman-Scott"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_notebook: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(caret)
library(xgboost)
library(SHAPforxgboost)
library(Matrix)
library(reticulate)
library(doParallel)
```

## LOAD DATA

```{r load-data}
df <- readRDS("../data/models/social-risk-crash-rate-data.rds") %>%
    select(-geoid, -total_population, -borough)
glimpse(df)
```

## HOT-ENCODE `YEAR`

I will treat `year` as a factor and one-hot encode it.

```{r hot-encode-year}
df$year <- as.factor(df$year)
year_dummies <- model.matrix(~ year - 1, data = df)

df <- cbind(df[ , !(names(df) %in% c("year"))], year_dummies)

glimpse(df)
```
## PREPARE TARGET

We will remove all possible target variables and keep only one per model training.

```{r single-target-df}
# Choose your target variable (e.g., crash rate per 1,000 residents)
target_var <- "crash_rate_per_1000"

# Remove all target variables except selected
cols_to_remove <- grep("crash|_injured|_killed|per_1000", 
                       names(df), 
                       value = TRUE)
cols_to_remove <- setdiff(cols_to_remove, target_var) # keep this column

df <- df %>% select(-all_of(cols_to_remove),)

# Create feature matrix and target vector
X <- df %>% select(-one_of(target_var))
y <- df[[target_var]]

```


## TRAIN/TEST SPLIT

```{r train-test-split}
# Set seed
set.seed(2025)

# Split by index
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_index, ]
y_train <- y[train_index]
X_test <- X[-train_index, ]
y_test <- y[-train_index]

# Convert to xgb.DMatrix
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)
```

## HYPERPARAMETER TUNING

**What This Does**
- Uses **Râ€™s `xgboost::xgb.cv()`** to evaluate each parameter set.
- Optuna (Python) handles the search space and Bayesian optimization.
- The final best parameters are applied to fit the `final_model`.
- **Search space:** Instead of predefined grids, `trial$suggest_float()` and `trial$suggest_int()` explore a range of values.
- **Best parameters:** `study$best_params` holds the optimal hyperparameters.

```{r tune-params}

reticulate::use_virtualenv("r-reticulate", required = TRUE)
optuna <- import("optuna")

# Define Optuna objective function
objective <- function(trial) {
  # Suggest parameters
  params <- list(
    booster = "gbtree",
    eta = trial$suggest_float("eta", 0.01, 0.3),
    max_depth = trial$suggest_int("max_depth", 3, 10),
    gamma = trial$suggest_float("gamma", 0, 5),
    colsample_bytree = trial$suggest_float("colsample_bytree", 0.5, 1),
    min_child_weight = trial$suggest_int("min_child_weight", 1, 10),
    subsample = trial$suggest_float("subsample", 0.5, 1),
    objective = "reg:squarederror",
    eval_metric = "rmse"
  )

  # Run cross-validation
  cv <- xgb.cv(
    params = params,
    data = dtrain,
    nrounds = 200,
    nfold = 5,
    early_stopping_rounds = 20,
    verbose = 0
  )

  # Return the best RMSE
  return(min(cv$evaluation_log$test_rmse_mean))
}

# Set Seed
set.seed(2025)

# Create and run Optuna study
study <- optuna$create_study(direction = "minimize")
study$optimize(objective, n_trials = 50)

# Get best parameters
best_params <- study$best_params
print(best_params)
```

## TRAIN MODEL

```{r train-model}
# Set seed
set.seed(2025)

# Training with parallel processing
final_model <- xgb.train(
  params = list(
    eta = best_params$eta,
    max_depth = best_params$max_depth,
    gamma = best_params$gamma,
    colsample_bytree = best_params$colsample_bytree,
    min_child_weight = best_params$min_child_weight,
    subsample = best_params$subsample,
    objective = "reg:squarederror",
    eval_metric = "rmse"
  ),
  data = dtrain,
  nrounds = 1000,
  watchlist = list(train = dtrain, test = dtest),
  early_stopping_rounds = 20,
  verbose = 1,
  nthread = detectCores() - 1
)
```


## SAVE MODEL

```{r save-model}
# Create directory if it doesn't exist
if (!dir.exists("../data/models")) {
  dir.create("../data/models", recursive = TRUE)
}

# Save the final XGBoost model
saveRDS(final_model, file = "../data/models/base_model.rds")

# Save the best parameters
saveRDS(best_params, file = "../data/models/base_best_params.rds")

cat("Model and parameters saved to ../data/models/")
```

## MODEL EVALUATION

```{r eval-model}
# Set seed
set.seed(2025)

# Predict on test set
preds <- predict(final_model, as.matrix(X_test))

# Calculate RMSE
rmse <- sqrt(mean((y_test - preds)^2))
cat("Test RMSE:", rmse)

# Plot predicted vs. actual
data.frame(actual = y_test, predicted = preds) %>%
  ggplot(aes(x = actual, y = predicted)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  theme_minimal() +
  labs(title = "Predicted vs Actual Crash Rates")
```

## SHAP EXPLAINABILITY

```{r shap}
# Compute SHAP values
shap_values <- shap.values(xgb_model = final_model, X_train = as.matrix(X_train))
shap_long <- shap.prep(shap_contrib = shap_values$shap_score, X_train = as.matrix(X_train))

# SHAP summary plot
print(shap.plot.summary(shap_long))

if (!dir.exists("../report/plots")) {
  dir.create("../report/plots")
}

png("../report/plots/base_shap_summary_plot.png", width = 1200, height = 800)
shap.plot.summary(shap_long)
dev.off()
```

## INSPECT TREE ENSEMBLE

```{r inspect-tree}
xgb.plot.tree(model = final_model, trees = 0)
xgb.plot.tree(model = final_model, trees = 1)
xgb.plot.tree(model = final_model, trees = 2)
xgb.dump(final_model, with_stats = TRUE)

xgb.plot.multi.trees(model = final_model)
```
