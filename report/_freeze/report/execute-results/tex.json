{
  "hash": "98f49435126e7ba15611bd55de4391e4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting Auto Insurance Risk Using Gradient Boosting\"\nsubtitle: \"Analyzing Socio-Economic Factors in Car Crashes for New York City\"\n\nauthor:\n  - name: AJ Strauman-Scott\n    orcid: \"0009-0000-9091-0697\"\n    email: {ajstraumanscott@pm.me}\n    \n    affiliations:\n      - id: cuny\n        name: City University Of New York (CUNY)\n        department: Department of Data Science\n        city: New York City\n        country: United States of America\n\nabstract: |\n  This study explores the use of the gradient boosting model XGBoost to predict auto insurance risk by integrating socio-economic variables from publicly available data. By treating crash frequency as proxy for insurance claims, the project aims to identify key neighborhood-level factors influencing risk. The dataset, encompassing 13,518 census tract-by-year observations from 2018 to 2023, captures demographic, economic, housing, and commuting indicators alongside engineered interaction variables. Optuna hyperparameter tuning and SHAP-based explainability reveal that post-pandemic traffic dynamics, median gross rent, percent  of  a population in the labor-force, and the interaction of poverty with vehicle ownership are significant predictors of crash risk. While the model achieves moderate predictive accuracy ($R^2$ = 0.425), its interpretability highlights socio-economic disparities that influence urban traffic safety. The findings underscore the potential of open data-driven models for portfolio-level risk assessment and urban safety planning, while cautioning against direct use for individual underwriting due to fairness and legal concerns. \n\nkeywords: \n    - Gradient Boosting\n    - XGBoost\n    - SHAP explainability\n    - hyperparameter optimization\n    - auto insurance risk\n    - American Community Survey (ACS)\n    - NYC Open Data\n    - predictive modeling\n    - socio-economic predictors\n    - crash modeling\n\ndate: last-modified\nbibliography: bibliography.bib\n\nformat:\n  elsevier-pdf:\n    keep-tex: true\n    citation-style: authoryear \n---\n\n## Introduction {#sec-intro}\n\nAccurate insurance risk modeling is critical for setting fair premiums, mitigating losses, and ensuring financial stability within the insurance industry \\citep{henckaerts, clemente}. Predicting claim frequency supports pricing and enables insurers to manage portfolio-level risk and optimize resource allocation \\citep{mohamed}.\n\nNew York City (NYC) presents a complex urban environment where traffic risks are shaped by socio-economic factors, dense infrastructure, and scaling dynamics typical of large metropolitan areas \\citep{cabrera, bettencourt}. The availability of open datasets—such as NYC’s Motor Vehicle Collision (MVC) data and socio-economic indicators from the American Community Survey (ACS)—offers a unique opportunity to develop proxy models for insurance claim risk. These data sources provide detailed insights into crash frequency, commuting behaviors, and neighborhood-level demographics \\citep{adeniyi, brubacher}.\n\nTraditional actuarial methods, such as Generalized Linear Models (GLMs), have long been the foundation of risk pricing and underwriting due to their interpretability and regulatory acceptance \\citep{henckaerts}. However, GLMs are limited in their ability to capture non-linear relationships and interactions among complex predictors like socio-economic factors, urban infrastructure, and driving behavior \\citep{clemente}. These limitations are particularly pronounced in urban contexts, where crash risk is shaped by heterogeneous population dynamics and localized factors \\citep{cabrera, brubacher}.\n\nRecent studies and systematic reviews confirm that machine learning methods, particularly ensemble models like Gradient Boosting Machines (GBMs), outperform traditional GLMs for predicting both claim frequency and severity \\citep{clemente, mohamed, behboudi}. These models are capable of handling mixed data types (categorical and continuous) and capturing complex feature interactions that linear models often miss.\n\nTo address the interpretability challenge of “black box” ML models, SHAP (SHapley Additive exPlanations) offers a principled framework for feature attribution, allowing insurers and policymakers to understand both global feature importance and instance-level predictions \\citep{lundberg, dong, ning}. This combination of high-performance prediction and explainability provides a strong foundation for modern risk modeling \\citep{kim}.\n\nDespite the growing body of work applying GBMs to insurance modeling, few studies integrate publicly available crash data with socio-economic indicators to model claim-related risks specifically for the automotive insurance sector. Most research remains limited to proprietary policyholder data \\citep{henckaerts, mohamed}, while systematic reviews highlight that few studies combine open crash data with socio-economic indicators in insurance modeling \\citep{ali, behboudi}.\n\nThis study aims to integrate ACS socio-economic features with NYC MVC crash data to develop an explainable gradient boosting framework for measuring social risk for automotive insurance and urban policy. The ultimate goal is to identify key socio-economic, transportation, and demographic predictors that drive claim frequency, and to determine how much crash risk can be predicted solely by these factors using an XGBoost model, without incorporating weather, road pattern or individual-level driver data.\n\nThe remainder of this paper is organized as follows: Section 2 reviews prior work on ML in insurance risk modeling, model explainability, and literature gaps; Section 3 details the data sources, key metrics, modeling approach, and SHAP-based explainability; Section 4 reports the results including hyperparameter tuning results, model performance, and feature importance; Section 5 discusses the findings in relation to existing research and industry applications; and Section 6 concludes with key contributions, limitations, and directions for future research.\n\n## Related Work {#sec-lit-review}\n\n### **Machine Learning in Insurance Risk Modeling**\n\nThe transition from traditional actuarial models such as Generalized Linear Models (GLMs) to machine learning (ML) approaches has marked a significant evolution in insurance risk modeling. GLMs have historically served as the backbone for pricing and claim prediction due to their interpretability and regulatory acceptance. However, they are limited by their linearity and inability to naturally capture complex interactions and nonlinear relationships among predictors, such as driver demographics, vehicle characteristics, socio-economic factors, and driving behavior. As \\citet{clemente} note, while GLMs remain effective for modeling claim severity with smaller and noisier datasets, they often underperform compared to ensemble methods when modeling claim frequency, where nonlinearities and heterogeneous risk patterns are prevalent. Similarly, \\citet{jonkheijm} demonstrated that tree-based models, especially XGBoost, substantially improved predictive accuracy over linear regression, particularly when incorporating both actuarial features (e.g., policyholder age, vehicle value) and behavioral indicators.\n\nRecent studies have validated the predictive superiority of ML methods—such as random forests, GBMs, and neural networks—over traditional actuarial models. GBMs, such as XGBoost and LightGBM, have emerged as particularly effective tools in auto insurance risk modeling \\citep{henckaerts}. Their iterative boosting framework enables them to handle mixed data types (categorical and continuous) and capture intricate patterns that GLMs and single decision trees may miss. \\citet{clemente} applied gradient boosting to both claim frequency and severity modeling, demonstrating significant performance gains in frequency prediction over Poisson-based GLMs. Similarly, \\citet{jonkheijm} employed XGBoost for forecasting individual claim amounts, outperforming both regression trees and random forests.\n\n### **Use of Crash and Socio-Economic Data**\n\nCrash data has been widely recognized as a reliable proxy for insurance claim frequency, given the direct link between the occurrence of traffic accidents and subsequent claims filed by policyholders. Studies utilizing police crash reports, telematics, and open transportation datasets consistently demonstrate strong correlations between crash frequency and insurance risk metrics \\citep{takale}. \n\nThe integration of socio-economic features—including income levels, commuting patterns, vehicle ownership rates, and population density—has been shown to enhance the explanatory power of crash and claim prediction models. For example, \\citet{adeniyi} utilized a decade of NYC crash data (2013–2023) to identify key predictors of accident severity—such as unsafe speed, alcohol involvement, and adverse weather—which align closely with the variables insurers use to model claim likelihood. Similarly, \\citet{dong} applied boosting-based ensemble models to traffic injury severity prediction, finding that vehicle type, collision mode, and environmental conditions strongly influenced both injury outcomes and, by extension, potential claim costs. \\citet{brubacher} conducted a geospatial analysis of 10 years of crashes in British Columbia and found that regions with lower income and higher socio-economic deprivation exhibited higher rates of pedestrian crashes, severe injuries, and fatalities, reflecting disparities in road safety linked to infrastructure quality and enforcement intensity. \\citet{cabrera} expanded on this by identifying superlinear scaling of road accidents in urban areas, where higher population densities led to disproportionate increases in crash frequency, especially for minor collisions. These findings are directly relevant for insurers, as they imply that socio-economic and urban structural factors—such as commuting patterns or access to public transit—can serve as proxies for underlying risk exposure.\n\nUrban-focused studies have further illuminated the unique risk dynamics in metropolitan environments like New York City, Chicago, and London, where complex traffic patterns, dense road networks, and high pedestrian activity elevate accident risk. \\citet{adeniyi} analyzed NYC crash data to show how the COVID-19 pandemic altered accident patterns, with fewer total crashes but an increase in injury severity due to higher vehicle speeds on less congested roads. \\citet{feng}, studying UK traffic data, emphasized the value of big data platforms and spatial clustering techniques (e.g., accident hotspot detection) to identify urban risk zones, a concept that parallels insurer efforts to assess region-based risk for underwriting.\n\n### **Explainability in GBM Models**\n\nIn high-stakes fields such as insurance pricing, underwriting, and claims management, the interpretability of ML models is not only a technical preference but also a regulatory and business requirement. Insurers must be able to justify rating factors and risk scores to regulators, policyholders, and internal stakeholders. Traditional actuarial models like GLMs are naturally interpretable due to their linear structure and explicit coefficient estimates. However, modern ML models—such as gradient boosting or neural networks—are often criticized as “black boxes,” complicating the explanation of predictions that influence financial decisions or customer premiums. Regulatory frameworks, including the EU’s General Data Protection Regulation (GDPR) and U.S. state-level insurance guidelines, increasingly require transparency in algorithmic decision-making, further amplifying the need for explainable AI (shortened to XAI). \\citet{henckaerts} further underscore this, showing that variable importance plots and partial dependence plots (PDPs) can yield actionable insights into driver and policyholder risk factors, blending predictive power with interpretability.\n\nAmong XAI methods, SHAP (SHapley Additive exPlanations) has become the state-of-the-art framework for interpreting complex ML models. Developed by \\citet{lundberg}, SHAP is grounded in cooperative game theory, assigning each feature a Shapley value that quantifies its contribution to individual predictions. Unlike traditional feature importance metrics—such as Gini importance in random forests or split gain in XGBoost—SHAP accounts for both main effects and feature interactions, offering a consistent and additive explanation of how variables drive model outputs.\n\nIn the insurance domain, SHAP has been widely applied to interpret models for claims prediction, fraud detection, and risk scoring. \\citet{dong} used SHAP in conjunction with boosting-based models (LightGBM and CatBoost) to analyze the contribution of driver age, vehicle type, and collision type to injury severity predictions, providing insights that aligned with domain expertise. Similarly, \\citet{ning} demonstrated how Shapley Variable Importance Cloud (ShapleyVIC) builds on SHAP principles to assess variable significance with uncertainty intervals, enabling fairer and more transparent risk predictions. \n\n### **Gaps in the Literature**\n\nWhile ML methods—particularly ensemble models like gradient boosting—have gained traction in insurance risk modeling, there is a notable absence of studies that combine socio-economic and crash data for claim risk prediction. Most existing research focuses on proprietary insurance datasets containing policyholder and vehicle information \\citep{clemente, henckaerts, jonkheijm}. This gap limits the development of robust, regionally sensitive models that capture the real-world interaction between crash frequency and socio-economic indicators.\n\n## Materials and Methods {#sec-methods}\n\n### **Data Sources**\n\nThe data sources and preprocessing steps are designed to replicate key factors used in actuarial risk models while incorporating broader socio-economic and regional variables.\n\n#### **Crash Data (Claim Proxies)**\n\nCrash data is obtained from the NYC Motor Vehicle Collisions (MVC) Open Data Portal, covering the years 2018–2023. These variables are well-documented predictors of both accident severity and insurance claims \\citep{adeniyi, dong}.\n\nCrash frequency was aggregated at the 2020 census tract level and normalized by tract-level population to compute crashes per 1,000 resident. This metric will replace claim frequency \\citep{brubacher}.\n\n#### **Socio-Economic Data (ACS Features)**\n\nSocio-economic variables are drawn from the ACS 5-year estimates (2018–2023) at the 2020 census tract level. The variables include demographic composition , age distribution, and income indicators. Additional features include median gross rent, housing tenure, educational attainment, employment metrics, and transportation factors. The full table of ACS derived variables in available in @sec-appA.\n\n### **Key Metrics**\n\nThe primary risk metric, `crash_rate_per_1000`, measures the number of crashes per 1,000 residents in each census tract by year. This population-adjusted rate follows the methodology of studies that normalize crash counts by population to ensure fair comparisons of relative risk across areas with varying exposure levels \\citep{brubacher, cabrera}.\n\nThis response variable is modeled alongside the transformed and selected socio-economic and transportation variables detailed in @sec-appB. Together, these variables allow the model to capture both the exposure risk (frequency) and potential cost severity of accidents, aligning with the frameworks used in both insurance \\citep{clemente, henckaerts} and traffic safety research \\citep{dong}.\n\n#### **Preprocessing**\n\nCrash records from the NYC Open Data MVC dataset are spatially joined to 2020 Census Tracts using official census tract shapefiles. Annual summaries of total crashes, injuries, and fatalities are then aggregated by tract and normalized by tract-level population to compute annual  per-capita crash rates for each census tract in NYC for each year. \n\nThe ACS socio-economic data are harmonized to 2020 tract boundaries (via crosswalks for 2018–2019), binned into interpretable categories, and converted to percentages of total population where applicable. Interaction features-poverty rate and vehicle ownership, as well as unemployment rate and vehicle ownership-were engineered to capture compound effects on risk exposure. \n\nAfter examination, a subset of highly correlated variables were removed. Measures of poverty level and population above the poverty line, as well as employment and unemployment percentages, were closely tied to broader income and labor force indicators already included in the model, creating redundancy without improving predictive power. Similarly, metrics describing commuting alone by car and the distribution of vehicle ownership were binned to prevent issues from  being strongly interrelated. The precentage of female share of the population was excluded because of its near-perfect correlation with the male share of the population. The same was true for the share of high-income households, which closely overlapped with median income levels. \n\nNo categorical encoding besides `year` or feature standardization was performed, as all ACS predictors are expressed as continuous percentages or numeric values, and gradient boosting models (XGBoost) handle raw scales effectively \\citep{henckaerts}. However, the response variable - number of crashes per 1,000 people - was log-transformed to stabilize variance and reduce the impact of extreme values.\n\n### **Modeling Approach**\n\nXGBoost \\citep{xgboost} is chosen for its strong track record in insurance risk modeling and interpretability when combined with SHAP \\citep{dong}. This model selection aligns with studies comparing boosting frameworks for both frequency-severity modeling \\citep{henckaerts} and urban crash prediction \\citep{adeniyi}.\n\nModel performance was optimized with hypermater tuning using the automated Bayesian optimization framework Optuna \\citep{optuna}. This approach is supported by prior research showing that systematic hyperparameter optimization significantly improves boosting model accuracy \\citep{liu}.\n\nEach configuration of Optuna tuning was evaluated using spatial cross-validation at the borough level on the training data to balance bias and variance, ensuring that the model captured meaningful patterns without overfitting or overgeneralizing across geography.\n\n## Results\n\n### **Descriptive Statistics**\n\nThe dataset comprises 13,518 census tract–year observations from 2018 to 2023. Population counts vary widely across tracts, with a median of approximately 42,979 residents and extremes ranging from fewer than 100 to over 220,000. \n\n![Crash Rate by Borough, by Year](plots/crash_rate_by_borough_plot.png){#fig-crashes}\n\nIn @fig-crashes, crash rate by borough shows a clear downward trend in crash rates across all boroughs over time, with a sharp reduction during the COVID-19 pandemic period (2020) and gradual stabilization afterward. Bronx and Queens consistently show higher crash rates per 1,000 residents compared to Staten Island and Manhattan. \n\n![Crash Rate in 2018 vs 2022](plots/2018_2022_comparison.png){#fig-heatmap}\n\nThe geospatial heatmaps in @fig-heatmap illustrate how crash rates are spatially clustered within the city. In 2018, high crash rates were concentrated in central Brooklyn, the South Bronx, and sections of northern Manhattan, while in 2022, these hotspots persisted but appeared less intense overall, consistent with the downward temporal trend across both total and commuting population post-Covid-19 pandemic. Applying the same color scale across both maps, it is evident that most census tracts saw a reduction in crash intensity, although isolated high-risk corridors remain.\n\n### **Hyperperameter Tuning**\n\nOptuna hyperparameter tuning \\citep{optuna} enhanced the predictive accuracy and generalization capability of the XGBoost model. The final configuration represents a balance between model complexity and overfitting risk, as determined by performance on the training and validation subsets.\n\n| Action | Parameter  | Value     |\n|---------|-----------|-----------|\n| Learning Rate | `eta`  | 0.2586295  |\n| Tree Depth | `max_depth`  | 12 |\n| Row Sampling | `subsample` | 0.9921435 |\n| Feature Sampling | `colsample_bytree` | 0.5823858 |\n| Minimum Child Weight | `min_child_weight` | 4 |\n| Minimum Loss Reduction  | `gamma` | 0.08316905 |\n| L2 Regularization | `lambda` | 4.260907 |\n| L1 Regularization | `alpha` | 0.2049112 |\n\n: Optimal Parameters as per Optuna\n\nThe tree depth of 12 allows the model to make much deeper splits, capturing highly granular interactions between socio-economic and crash-related variables. While deep trees can increase the risk of overfitting, this is balanced by the other constraints in the model. The minimum child weight places a threshold on the minimum sum of instance weights required to create a split. This prevents the model from building branches that explain only a small subset of observations, helping to maintain focus on broader patterns in the data.\n\nThe learning rate is moderate, enabling the model to update predictions in steady increments without being overly aggressive. This value strikes a balance between convergence speed and stability.\n\nThe row sampling rate (`subsample = 0.992`) indicates that nearly all rows are used in each boosting iteration, which reduces variance but increases the risk of memorizing noise in the data. In contrast, the feature sampling rate (`colsample_bytree = 0.582`) introduces meaningful randomness by selecting slightly more than half of the features for each tree.\n\nRegularization parameters, `lambda` (L2) and `alpha` (L1), apply moderate constraints to the feature weights, discouraging overly complex or extreme splits without overly penalizing flexibility. The minimum loss reduction (`gamma` = 0.083) is relatively low, which allows the model to explore more potential splits during training. This can enhance the model’s ability to capture subtle relationships between features.\n\n### **Model Performance**\n\nThe XGBoost model achieved robust predictive performance on the holdout test set, with the following metrics:\n\n| Metric  | Score     |\n|---------|-----------|\n| RMSE  | 0.3396146  |\n| MAE   | 0.2422538 |\n| $R^2$ | 0.425285 |\n\n: XGBOOST Model Evaluation Metrics\n\nThese metrics indicate solid predictive performance with tight error bounds relative to the normalized crash rate values. An RMSE of 0.34 suggests that predictions deviate by approximately 0.34 crashes per 1,000 residents on average, which is a low margin of error given the variability across census tracts. The MAE of 0.24 confirms that typical prediction errors remain well below half a crash per 1,000 residents.\n\nMost critically, the $R^2$ value of 0.43 means the model explains approximately 43% of the variance in crash rates, a strong result for a model built only on socio-economic and transportation predictors, without direct behavioral or vehicle-level data.\n\n![Residual Diagnostic Plots](plots/residuals_diagnostic_grid.png)\n\n#### **Residuals**\n\nThe predicted vs. actual values plot shows that while the model generally tracks the trend of observed crash rates, there is a clear cone-shaped spread. This indicates that predictions tend to be compressed toward the mean—with underestimation for high crash-rate tracts and overestimation for low crash-rate tracts. This is a common behavior for gradient boosting models trained on noisy data, where extreme values are smoothed due to ensemble averaging.\n\nThe residuals vs. predicted values plot also displays the same cone shape. This pattern suggests variance increases at the extremes, meaning the model is less confident and less accurate when predicting very high or very low crash rates.\n\nThe residual distribution plot remains centered around zero with a sharp peak, showing minimal systemic bias. \n\n### **Feature Importance (SHAP)**\n\n![Global Feature Importance with SHAP](plots/pdp/pdp_grid.png)\n\nThe SHAP plot highlights the six most influential variables shaping crash risk predictions in the XGBoost model. These features reveal complex, non-linear relationships with crash rates:\n\n**Post-Pandemic Indicator**\n\nThe pre vs. post-COVID-19 variable shows a sharp drop in predicted crash rates after the pandemic began, consistent with the reduction in traffic volumes and changes in driving patterns observed citywide, also observed by \\citet{adeniyi}.\n\n**Aging Population**\n\nCrash risk declines as the proportion of residents aged 65 and older increases up to roughly 2%, likely due to reduced driving exposure among elderly populations. Beyond this point, the trend stabilizes, suggesting diminishing marginal effects.\n\n**Graduate Degree Share**\n\nThe percentage of residents with graduate degrees exhibits a steady downward relationship with crash risk. Higher educational attainment may correlate with neighborhoods that have better infrastructure or lower exposure to high-risk driving.\n\n**Working Population**\n\nThe PDP for labor force participation indicates a non-linear trend. Crash risk decreases slightly between 2% and 5% workforce participation but increases sharply as participation exceeds 5%, reflecting higher commuter activity and vehicle miles traveled.\n\n**Median Income**\n\nMedian income shows a non-monotonic relationship: risk is elevated in very low-income areas, dips in middle-income neighborhoods, and then rises again at high-income levels, possibly reflecting denser, high-cost urban areas with complex traffic dynamics.\n\n**Medium Commute Share**\nThe percentage of residents with medium-length commutes (15–30 minutes) is strongly associated with higher crash risk once it exceeds approximately 3%. This likely reflects regions with heavier traffic flow and greater exposure to collisions.\n\n## Discussion\n\nThe XGBoost model achieved a notable performance with an $R^2$ of 0.43, meaning it explains over 40% of the variance in crash rates across New York City census tracts. This is a strong result given the absence of driver-level or vehicle-level data and the inherently stochastic nature of traffic collisions. The low RMSE (0.34 crashes per 1,000 residents) and MAE (0.24) indicate that the model’s predictions are generally close to observed values, with small, consistent errors across most census tracts.\n\nFrom a social risk modeling perspective, the results highlight the importance of broader socio-economic and transportation-related conditions in shaping crash risk. The SHAP analysis reveals that the post-pandemic indicator is the single strongest predictor, showing a significant shift in traffic patterns starting in 2020. Reduced congestion but increased vehicle speeds during and after the pandemic contributed to changes in crash risk that conventional actuarial models might fail to capture.\n\nKey socio-economic variables also play prominent roles. Median income exhibits a non-linear relationship with crash risk, with both very low-income and high-income areas seeing higher risk—likely due to a mix of infrastructure disparities and dense traffic flows. Labor force participation and commute-related metrics, particularly the share of medium-length commutes, are also strongly predictive, indicating that commuter activity and roadway exposure remain fundamental drivers of crash frequency. The negative association between graduate degree share and crash risk may reflect safer traffic environments or reduced reliance on driving in these neighborhoods.\n\nThese findings emphasize the interplay between economic conditions, urban infrastructure, and mobility behaviors in shaping neighborhood-level crash risks. High-income and low-income areas may share common risk factors, such as higher vehicle ownership rates, complex traffic flows, or limited access to safe pedestrian infrastructure, albeit for different reasons. Similarly, commute-related variables underscore the importance of traffic volume and duration as core determinants of crash likelihood, suggesting that transportation policies—such as investment in public transit or traffic-calming measures—could meaningfully reduce risks\n\nThe cone-shaped residual patterns suggest that while the model performs well across the midrange of crash rates, it tends to compress predictions toward the mean—underestimating high-risk tracts and overestimating low-risk ones. This limitation is typical for tree-based ensembles trained on data with sparse extremes, but it indicates an opportunity for refinement through log-transformed targets or quantile regression techniques.\n\nAlthough the model provides actionable insights into spatial risk patterns, its direct use for individual insurance pricing is neither appropriate nor ethical. Socio-economic and demographic factors, while predictive, are not permissible as rating factors due to potential for proxy discrimination. Instead, these findings are better suited for identifying high-risk areas for targeted safety interventions, urban planning efforts, or high-level portfolio analysis.\n\n## Conclusions and Future Work\n\nThis study demonstrates that gradient boosting models, when combined with open crash and socio-economic data, can produce accurate and interpretable models of neighborhood-level crash risk in New York City. By identifying factors such as post-pandemic traffic changes, income patterns, educational attainment, and commuting behaviors, the model offers a rich, data-driven understanding of how socio-economic context influences crash frequency.\n\nHowever, the model’s focus on aggregated census tract data and its moderate $R^$2 highlight the limits of what can be achieved without driver-specific or telematics data. While it effectively captures macro-level trends, it is not a replacement for traditional actuarial models but rather a complementary tool for exploring spatial and demographic risk factors.\n\nFuture research should focus on three fronts: (1) integrating behavioral data, such as telematics, and weather patterns-among other possible additions-to bridge the gap between macro-level socio-economic patterns and micro-level driving behavior; (2) developing fairness-aware modeling approaches to mitigate bias from socio-economic proxies; and (3) exploring temporal extensions that incorporate evolving risk factors, including post-pandemic traffic patterns and climate-related hazards. These directions will help transition from descriptive social risk modeling to actionable, ethically sound insurance applications.\n\n## Appendix A: ACS Variables {#sec-appA}\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: ACS tables and derived variables.\n\n|ACS    |Description             |Derived.Variables                                                                                     |\n|:------|:-----------------------|:-----------------------------------------------------------------------------------------------------|\n|B01001 |Age and Sex             |total_population, male_population, female_population, age_under_18, age_18_34, age_35_64, age_65_plus |\n|B08301 |Transportation to Work  |drive_alone, carpool, public_transit, walk, bike, work_from_home                                      |\n|B08303 |Travel Time to Work     |commute_short, commute_medium, commute_long                                                           |\n|B19001 |Household Income        |income_under_25k, income_25k_75k, income_75k_plus, median_income                                      |\n|B25010 |Average Household Size  |average_household_size                                                                                |\n|B25044 |Vehicles Available      |no_vehicle, one_vehicle, two_plus_vehicles                                                            |\n|C24010 |Occupation              |occupation variables (aggregated)                                                                     |\n|C24030 |Industry                |industry variables (aggregated)                                                                       |\n|B15003 |Education               |less_than_hs, hs_diploma, some_college, associates_degree, bachelors_degree, graduate_degree          |\n|B17001 |Poverty Status          |below_poverty, above_poverty, poverty_rate                                                            |\n|B02001 |Race                    |white_population, black_population, asian_population                                                  |\n|B03002 |Hispanic or Latino      |hispanic_population                                                                                   |\n|B16005 |Language Spoken at Home |foreign_born                                                                                          |\n|B23025 |Employment Status       |in_labor_force, employed, unemployed, not_in_labor_force, unemployment_rate                           |\n|B25064 |Median Gross Rent       |median_gross_rent                                                                                     |\n\n\n:::\n:::\n\n\n## Appendix B: Variables Modeled {#sec-appB}\n\n::: {.cell tbl-cap='Key variables, descriptions, and transformations in the final dataset.'}\n::: {.cell-output-display}\n\n\nTable: Table 2: Key variables, descriptions, and transformations in the final dataset.\n\n|Variable       |Description                          |Type                                                    |Transformation  |\n|:--------------|:------------------------------------|:-------------------------------------------------------|:---------------|\n|Demographic    |pct_male_population                  |Men                                                     |Percentage      |\n|Demographic    |pct_white_population                 |Identifying as white                                    |Percentage      |\n|Demographic    |pct_black_population                 |Identifying as black                                    |Percentage      |\n|Demographic    |pct_asian_population                 |Identifying as Asian                                    |Percentage      |\n|Demographic    |pct_hispanic_population              |Identifying as Hispanic/Latino                          |Percentage      |\n|Demographic    |pct_foreign_born                     |Foreign-born                                            |Percentage      |\n|Age            |pct_age_under_18                     |Under 18                                                |Percentage      |\n|Age            |pct_age_18_34                        |Aged 18-34                                              |Percentage      |\n|Age            |pct_age_35_64                        |Aged 35-64                                              |Percentage      |\n|Age            |pct_age_65_plus                      |Aged 65 and above                                       |Percentage      |\n|Income/Poverty |median_income                        |Median household income (inflation-adjusted)            |Raw value (USD) |\n|Income/Poverty |pct_income_under_25k                 |Households earning less than $25,000                    |Percentage      |\n|Income/Poverty |pct_income_25k_75k                   |Households earning $25,000-$75,000                      |Percentage      |\n|Income/Poverty |pct_below_poverty                    |Below the poverty line                                  |Percentage      |\n|Housing        |median_gross_rent                    |Median gross rent (USD)                                 |Raw value (USD) |\n|Housing        |pct_owner_occupied                   |Owner-occupied housing units                            |Percentage      |\n|Housing        |pct_renter_occupied                  |Renter-occupied housing units                           |Percentage      |\n|Education      |pct_less_than_hs                     |Less than high school education                         |Percentage      |\n|Education      |pct_hs_diploma                       |High school diploma                                     |Percentage      |\n|Education      |pct_some_college                     |Some college education                                  |Percentage      |\n|Education      |pct_associates_degree                |Associate's degree                                      |Percentage      |\n|Education      |pct_bachelors_degree                 |Bachelor's degree                                       |Percentage      |\n|Education      |pct_graduate_degree                  |Graduate or professional degree                         |Percentage      |\n|Employment     |pct_in_labor_force                   |In the labor force                                      |Percentage      |\n|Employment     |unemployment_rate                    |Unemployment rate                                       |Percentage      |\n|Transport      |pct_commute_short                    |Commute under 15 minutes                                |Percentage      |\n|Transport      |pct_commute_medium                   |Commute between 15-30 minutes                           |Percentage      |\n|Transport      |pct_commute_long                     |Commute longer than 30 minutes                          |Percentage      |\n|Transport      |pct_carpool                          |By carpool                                              |Percentage      |\n|Transport      |pct_public_transit                   |By public transit                                       |Percentage      |\n|Transport      |pct_walk                             |By walking                                              |Percentage      |\n|Transport      |pct_bike                             |By biking                                               |Percentage      |\n|Transport      |pct_work_from_home                   |Working from home                                       |Percentage      |\n|Transport      |pct_vehicle                          |Owns a vehicle                                          |Percentage      |\n|Engineered     |post_pandemic                        |Post-pandemic indicator (1 = 2020 and later)            |Binary          |\n|Engineered     |poverty_vehicle\n    _interaction      |Interaction term: poverty rate × vehicle ownership      |Interaction     |\n|Engineered     |unemployment_vehicle\n    _interaction |Interaction term: unemployment rate × vehicle ownership |Interaction     |\n|Year           |year2018                             |Year dummy: 2018                                        |Indicator       |\n|Year           |year2019                             |Year dummy: 2019                                        |Indicator       |\n|Year           |year2020                             |Year dummy: 2020                                        |Indicator       |\n|Year           |year2021                             |Year dummy: 2021                                        |Indicator       |\n|Year           |year2022                             |Year dummy: 2022                                        |Indicator       |\n|Year           |year2023                             |Year dummy: 2023                                        |Indicator       |\n\n\n:::\n:::\n\n\n## References",
    "supporting": [
      "report_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}