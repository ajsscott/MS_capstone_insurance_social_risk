% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  number,
  review,
  3p]{elsarticle}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 
\usepackage[]{natbib}
\bibliographystyle{elsarticle-harv}


\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\journal{elsarticle}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Predicting Auto Insurance Risk Using Gradient Boosting},
  pdfauthor={AJ Strauman-Scott},
  pdfkeywords={Gradient Boosting, XGBoost, LightGBM, SHAP
explainability, hyperparameter optimization, auto insurance
risk, American Community Survey (ACS), NYC Open Data, predictive
modeling},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\setlength{\parindent}{6pt}
\begin{document}

\begin{frontmatter}
\title{Predicting Auto Insurance Risk Using Gradient
Boosting \\\large{Analyzing Socio-Economic and Crash Data in New York
City} }
\author[1]{AJ Strauman-Scott%
%
}
 \ead{ajstraumanscott@pm.me} 

\affiliation[1]{organization={City University Of New York
(CUNY), Department of Data Science},city={New York City},country={United
States of America},countrysep={,},postcode={11212},postcodesep={}}

\cortext[cor1]{Corresponding author}

        
\begin{abstract}
PUT AN ABSTRACT HERE!!
\end{abstract}





\begin{keyword}
    Gradient Boosting \sep XGBoost \sep LightGBM \sep SHAP
explainability \sep hyperparameter optimization \sep auto insurance
risk \sep American Community Survey (ACS) \sep NYC Open Data \sep 
    predictive modeling
\end{keyword}
\end{frontmatter}
    

\section{1. Introduction}\label{introduction}

Accurate insurance risk modeling is critical for setting fair premiums,
mitigating losses, and ensuring financial stability within the insurance
industry \citep[\citep{clemente}]{henckaerts}. Predicting claim
frequency and severity not only supports pricing but also enables
insurers to manage portfolio-level risk and optimize resource allocation
\citep{mohamed}.

New York City (NYC) presents a complex urban environment where traffic
risks are shaped by socio-economic factors, dense infrastructure, and
scaling dynamics typical of large metropolitan areas
\citep[\citep{bettencourt}]{cabrera}. The availability of open
datasets---such as NYC's Motor Vehicle Collision (MVC) data and
socio-economic indicators from the American Community Survey
(ACS)---offers a unique opportunity to develop proxy models for
insurance claim risk. These data sources provide detailed insights into
crash frequency, injury severity, commuting behaviors, and
neighborhood-level demographics \citep[\citep{brubacher}]{adeniyi}.

Traditional actuarial methods, such as Generalized Linear Models (GLMs),
have long been the foundation of risk pricing and underwriting due to
their interpretability and regulatory acceptance \citep{henckaerts}.
However, GLMs are limited in their ability to capture non-linear
relationships and interactions among complex predictors like
socio-economic factors, urban infrastructure, and driving behavior
\citep{clemente}. These limitations are particularly pronounced in urban
contexts, where crash risk is shaped by heterogeneous population
dynamics and localized factors \citep[\citep{brubacher}]{cabrera}.

There is a growing need for data-driven approaches that can flexibly
incorporate diverse predictors---such as open crash data and
socio-economic variables---while addressing the complex temporal and
spatial patterns of accidents highlighted in recent reviews
\citep[\citep{behboudi}]{grigorev}. Recent studies and systematic
reviews confirm that machine learning (ML) methods, particularly
ensemble models like Gradient Boosting Machines (GBMs), XGBoost, and
LightGBM, outperform traditional GLMs for predicting both claim
frequency and severity \citep[\citep{mohamed},
\citep{behboudi}]{clemente}. These models are capable of handling mixed
data types (categorical and continuous) and capturing complex feature
interactions that linear models often miss.

To address the interpretability challenge of ``black box'' ML models,
SHAP (SHapley Additive exPlanations) offers a principled framework for
feature attribution, allowing insurers and policymakers to understand
both global feature importance and instance-level predictions
\citep[\citep{dong}, \citep{ning}]{lundberg}. This combination of
high-performance prediction and explainability provides a strong
foundation for modern risk modeling, as demonstrated in other domains
such as maritime safety where interpretable models like SHAP have been
applied \citep{kim}.

Despite the growing body of work applying ML to insurance modeling, few
studies integrate publicly available crash data with socio-economic
indicators to model claim-related risks. Most research remains limited
to proprietary policyholder data \citep[\citep{mohamed}]{henckaerts},
while systematic reviews highlight that few studies combine open crash
data with socio-economic indicators in insurance modeling
\citep[\citep{behboudi}]{ali}.

This study aims to integrate ACS socio-economic features with NYC MVC
crash data to develop an explainable gradient boosting framework. The
ultimate goal is to identify key socio-economic and transportation
predictors that drive claim frequency and severity proxies, offering
insights for both insurers and urban policymakers.

The remainder of this paper is organized as follows: Section 2 reviews
prior work on machine learning in insurance risk modeling, crash and
socio-economic data, geospatial analytics, model explainability, and
literature gaps; Section 3 details the data sources, key metrics,
modeling approach, and SHAP-based explainability; Section 4 reports the
results including model performance, feature importance, and geospatial
patterns; Section 5 discusses the findings in relation to existing
research and industry applications; and Section 6 concludes with key
contributions, limitations, and directions for future research.

\section{2. Related Work}\label{related-work}

\subsection{\texorpdfstring{\textbf{2.1 Machine Learning in Insurance
Risk
Modeling}}{2.1 Machine Learning in Insurance Risk Modeling}}\label{machine-learning-in-insurance-risk-modeling}

The transition from traditional actuarial models such as Generalized
Linear Models (GLMs) to machine learning (ML) approaches has marked a
significant evolution in insurance risk modeling. GLMs have historically
served as the backbone for pricing and claim prediction due to their
interpretability and regulatory acceptance. However, they are limited by
their linearity and inability to naturally capture complex interactions
and nonlinear relationships among predictors, such as driver
demographics, vehicle characteristics, socio-economic factors, and
driving behavior. As \citep{clemente} note, while GLMs remain effective
for modeling claim severity with smaller and noisier datasets, they
often underperform compared to ensemble methods when modeling claim
frequency, where nonlinearities and heterogeneous risk patterns are
prevalent. Similarly, \citep{jonkheijm} demonstrated that tree-based
models, especially XGBoost, substantially improved predictive accuracy
over linear regression, particularly when incorporating both actuarial
features (e.g., policyholder age, vehicle value) and behavioral
indicators.

Recent studies have validated the predictive superiority of ML
methods---such as random forests, gradient boosting machines (GBM), and
neural networks---over traditional actuarial models. Gradient boosting
methods, such as XGBoost and LightGBM, have emerged as particularly
effective tools in auto insurance risk modeling \citep{henckaerts}.
Their iterative boosting framework enables them to handle mixed data
types (categorical and continuous) and capture intricate patterns that
GLMs and single decision trees may miss. \citep{clemente} applied
gradient boosting to both claim frequency and severity modeling,
demonstrating significant performance gains in frequency prediction over
Poisson-based GLMs. Similarly, \citep{jonkheijm} employed XGBoost for
forecasting individual claim amounts, outperforming both regression
trees and random forests.

\subsection{\texorpdfstring{\textbf{2.2 Use of Crash and Socio-Economic
Data}}{2.2 Use of Crash and Socio-Economic Data}}\label{use-of-crash-and-socio-economic-data}

Crash data has been widely recognized as a reliable proxy for insurance
claim frequency and severity, given the direct link between the
occurrence of traffic accidents and subsequent claims filed by
policyholders. Studies leveraging police crash reports, telematics, and
open transportation datasets consistently demonstrate strong
correlations between crash frequency and insurance risk metrics
\citep{takale}. The integration of socio-economic features---including
income levels, commuting patterns, vehicle ownership rates, and
population density---has been shown to enhance the explanatory power of
crash and claim prediction models.

For example, \citep{adeniyi} utilized a decade of NYC crash data
(2013--2023) to identify key predictors of accident severity---such as
unsafe speed, alcohol involvement, and adverse weather---which align
closely with the variables insurers use to model claim likelihood.
Similarly, \citep{dong} applied boosting-based ensemble models to
traffic injury severity prediction, finding that vehicle type, collision
mode, and environmental conditions strongly influenced both injury
outcomes and, by extension, potential claim costs. \citep{brubacher}
conducted a geospatial analysis of 10 years of crashes in British
Columbia and found that regions with lower income and higher
socio-economic deprivation exhibited higher rates of pedestrian crashes,
severe injuries, and fatalities, reflecting disparities in road safety
linked to infrastructure quality and enforcement intensity.
\citep{cabrera} expanded on this by identifying superlinear scaling of
road accidents in urban areas, where higher population densities led to
disproportionate increases in crash frequency, especially for minor
collisions. These findings are directly relevant for insurers, as they
imply that socio-economic and urban structural factors---such as
commuting patterns or access to public transit---can serve as proxies
for underlying risk exposure.

Urban-focused studies have further illuminated the unique risk dynamics
in metropolitan environments like New York City, Chicago, and London,
where complex traffic patterns, dense road networks, and high pedestrian
activity elevate accident risk. \citep{adeniyi} analyzed NYC crash data
to show how the COVID-19 pandemic altered accident patterns, with fewer
total crashes but an increase in injury severity due to higher vehicle
speeds on less congested roads. \citep{feng}, studying UK traffic data,
emphasized the value of big data platforms and spatial clustering
techniques (e.g., accident hotspot detection) to identify urban risk
zones, a concept that parallels insurer efforts to assess region-based
risk for underwriting.

Collectively, these studies support the notion that combining crash data
with socio-economic indicators offers a powerful means of modeling
insurance claim frequency and severity. By integrating open data
sources---such as NYC's Vision Zero crash records and U.S.
Census-derived socio-economic attributes---researchers and insurers can
capture a more holistic view of driver risk behavior, infrastructure
quality, and regional safety disparities.

\subsection{\texorpdfstring{\textbf{2.3 Explainability in Machine
Learning
Models}}{2.3 Explainability in Machine Learning Models}}\label{explainability-in-machine-learning-models}

In high-stakes fields such as insurance pricing, underwriting, and
claims management, the interpretability of machine learning (ML) models
is not only a technical preference but also a regulatory and business
requirement. Insurers must be able to justify rating factors and risk
scores to regulators, policyholders, and internal stakeholders.
Traditional actuarial models like GLMs are naturally interpretable due
to their linear structure and explicit coefficient estimates. However,
modern ML models---such as gradient boosting or neural networks---are
often criticized as ``black boxes,'' complicating the explanation of
predictions that influence financial decisions or customer premiums.
Regulatory frameworks, including the EU's General Data Protection
Regulation (GDPR) and U.S. state-level insurance guidelines,
increasingly require transparency in algorithmic decision-making,
further amplifying the need for explainable AI (XAI). \citep{henckaerts}
further underscore this, showing that variable importance plots and PDPs
can yield actionable insights into driver and policyholder risk factors,
blending predictive power with interpretability.

Among XAI methods, SHAP (SHapley Additive exPlanations) has become the
state-of-the-art framework for interpreting complex ML models. Developed
by \citep{lundberg}, SHAP is grounded in cooperative game theory,
assigning each feature a Shapley value that quantifies its contribution
to individual predictions. Unlike traditional feature importance
metrics---such as Gini importance in random forests or split gain in
XGBoost---SHAP accounts for both main effects and feature interactions,
offering a consistent and additive explanation of how variables drive
model outputs.

Tools like SHAP allow practitioners to interpret complex models by
quantifying the contribution of each variable to the predictions.
Studies like \citep{mohamed} highlight the value of such
interpretability when using gradient boosting for pricing and fraud
detection, as insurers must justify rating factors for regulatory
compliance.

In the insurance domain, SHAP has been widely applied to interpret
models for claims prediction, fraud detection, and risk scoring.
\citep{dong} used SHAP in conjunction with boosting-based models
(LightGBM and CatBoost) to analyze the contribution of driver age,
vehicle type, and collision type to injury severity predictions,
providing insights that aligned with domain expertise. Similarly,
\citep{ning} demonstrated how Shapley Variable Importance Cloud
(ShapleyVIC) builds on SHAP principles to assess variable significance
with uncertainty intervals, enabling fairer and more transparent risk
predictions. These approaches not only improve trust in ML-driven
decision-making but also help insurers identify the most actionable risk
factors influencing claims.

\subsection{\texorpdfstring{\textbf{2.4 Gaps in the
Literature}}{2.4 Gaps in the Literature}}\label{gaps-in-the-literature}

While machine learning methods---particularly ensemble models like
gradient boosting---have gained traction in insurance risk modeling,
there is a notable absence of studies that combine socio-economic and
crash data for claim risk prediction. Most existing research focuses on
proprietary insurance datasets containing policyholder and vehicle
information \citep[\citep{henckaerts}, \citep{jonkheijm}]{clemente}.
However, publicly available crash datasets, such as NYC's Motor Vehicle
Collision (MVC) reports, and socio-economic features from the American
Community Survey (ACS) remain underutilized in insurance modeling. This
gap limits the development of robust, regionally sensitive models that
capture the real-world interaction between driving risk factors (e.g.,
crash frequency) and socio-economic indicators (e.g., income, commuting
patterns, and vehicle ownership rates). By integrating ACS data with
urban crash records, it becomes possible to construct granular,
location-aware risk models that better reflect variations in driving
exposure, infrastructure quality, and neighborhood-level risk factors.

\section{3. Materials and Methods}\label{materials-and-methods}

\subsection{\texorpdfstring{\textbf{3.1 Data Sources and
Preprocessing}}{3.1 Data Sources and Preprocessing}}\label{data-sources-and-preprocessing}

This study integrates publicly available crash data from New York City
with socio-economic features from the American Community Survey (ACS) to
develop a proxy model for insurance claim risk. The data sources and
preprocessing steps are designed to replicate key factors used in
actuarial risk models while incorporating broader socio-economic and
regional variables.

\subsubsection{\texorpdfstring{\textbf{Crash Data (Claim
Proxies)}}{Crash Data (Claim Proxies)}}\label{crash-data-claim-proxies}

Crash data is obtained from the NYC Motor Vehicle Collisions (MVC) Open
Data Portal, covering the years 2018--2023. Each record includes details
such as crash location, number of injuries and fatalities, vehicle type,
and contributing factors (e.g., driver behavior, environmental
conditions). These variables are well-documented predictors of both
accident severity and insurance claims \citep[\citep{dong}]{adeniyi}.

Crash frequency is aggregated at the borough and neighborhood level and
normalized by population to calculate crashes per 1,000 residents as a
proxy for claim frequency \citep{brubacher}. Injury and fatality counts
are used to compute injury-to-fatality ratios, which serve as a proxy
for claim severity, reflecting the expected cost burden of accidents
\citep{clemente}.

\subsubsection{\texorpdfstring{\textbf{Socio-Economic Data (ACS
Features)}}{Socio-Economic Data (ACS Features)}}\label{socio-economic-data-acs-features}

Socio-economic variables are drawn from the ACS 5-year estimates
(2018--2022) at the Neighborhood Tabulation Area (NTA) level, ensuring
consistency with NYC's geographic units for urban planning. Variables
include: Median income, age distribution, and educational attainment.
Mode share (car, subway, walking), average commute times, and vehicle
ownership rates. Household size, population density, and residential
occupancy patterns. These features have been shown to correlate with
traffic risk and claim likelihood, as evidenced by \citep{brubacher} and
\citep{cabrera}, who found that socio-economic deprivation and commuting
behaviors strongly influence crash frequency and severity.

\textbf{Preprocessing Steps}

Crash records are spatially joined to ACS NTAs using NYC Open Data
shapefiles. Derived metrics include per-capita crash frequency, vehicle
ownership ratios, and log-transformed density variables to capture
non-linear scaling effects \citep{cabrera}. Outliers (e.g., extreme
crash counts from anomalous events) are removed using interquartile
range thresholds. Missing socio-economic values are imputed using median
imputation.

Categorical variables (e.g., predominant commute mode) are one-hot
encoded, and continuous variables are standardized for input into
gradient boosting models \citep{henckaerts}.

This integrated dataset allows us to examine how socio-economic
conditions interact with crash patterns to inform insurance claim
frequency and severity modeling, addressing a gap in the literature
where these public data sources have rarely been combined
\citep[\citep{jonkheijm}]{mohamed}.

\subsection{3.2 Key Metrics}\label{key-metrics}

To model insurance risk in the absence of proprietary claims data,
publicly available crash data is used as a proxy for both claim
frequency and severity, a practice supported by prior research that
links traffic accidents directly to insurance losses
\citep[\citep{dong}]{adeniyi}.

The metric crashes per 1,000 residents is employed to approximate claim
frequency, following the approach of studies that normalized accident
counts by population density to assess relative risk across regions
\citep[\citep{cabrera}]{brubacher}. This population-adjusted measure
accounts for differences in exposure between neighborhoods or boroughs,
enabling fair comparisons of risk levels.

To estimate claim severity, we compute injury-to-fatality ratios based
on reported crashes, reflecting the likelihood and severity of bodily
harm resulting from incidents. This metric is consistent with the
severity-focused modeling frameworks used in both insurance
\citep[\citep{henckaerts}]{clemente} and traffic safety studies
\citep{dong}, where injury severity is treated as a key determinant of
financial impact.

\subsection{3.3 Modeling Approach}\label{modeling-approach}

To capture complex, non-linear relationships between socio-economic
variables (e.g., income, commuting patterns) and crash risk, we use
gradient boosting algorithms, specifically XGBoost and LightGBM.
Gradient boosting has consistently outperformed traditional GLMs and
linear models in insurance claim prediction and crash severity modeling
due to its ability to handle heterogeneous data types (categorical and
continuous) and capture feature interactions
\citep[\citep{mohamed}]{clemente}.

XGBoost is chosen for its strong track record in insurance risk modeling
and interpretability when combined with SHAP \citep{dong}, while
LightGBM is included for its efficiency on large datasets and superior
performance on high-cardinality categorical variables. This dual-model
approach aligns with studies comparing boosting frameworks for both
frequency-severity modeling \citep{henckaerts} and urban crash
prediction \citep{adeniyi}.

To optimize model performance, hyperparameter tuning is conducted using
Optuna, a state-of-the-art optimization framework leveraging Bayesian
search and pruning strategies to efficiently navigate large
hyperparameter spaces. This approach is supported by prior research
showing that systematic hyperparameter optimization significantly
improves boosting model accuracy \citep[\citep{liu}]{dong}.

\subsection{3.4 Explainability}\label{explainability}

Given the regulatory and operational need for transparent, explainable
models in insurance \citep[\citep{lundberg}]{henckaerts}, we employ SHAP
(SHapley Additive exPlanations) for both global and local feature
analysis. SHAP values are aggregated across the dataset to quantify
overall feature importance, revealing which socio-economic and
crash-related variables most influence predicted claim frequency and
severity.

\section{4. Results}\label{results}

\begin{itemize}
\tightlist
\item
  Model performance metrics (AUC, RMSE).
\item
  Feature importance rankings (SHAP).
\item
  Visual findings (maps, animations, dashboards).
\end{itemize}

\section{5. Conclusions and Future
Work}\label{conclusions-and-future-work}

\begin{itemize}
\tightlist
\item
  Key insights and implications for insurance risk modeling.
\item
  Limitations and suggested extensions (e.g., telematics, temporal
  models).
\item
  Potential improvements in visualization and explainability.
\end{itemize}


\renewcommand\refname{References}
\bibliography{bibliography.bib}



\end{document}
